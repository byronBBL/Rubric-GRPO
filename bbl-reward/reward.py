#!/usr/bin/env python3
"""
Rubric Reward Service

è¿™ä¸ªæœåŠ¡ä½¿ç”¨ vLLM åŠ è½½ reward modelï¼Œå¯¹æ”¶åˆ°çš„æ•°æ®è¿›è¡Œè¯„åˆ†ã€‚
æ•°æ®æ ¼å¼: {rubric: rubric, question: question, response: response}
è¿”å›æ ¼å¼: [{score: ..., weight: ...}, ...]
"""

import json
import re
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from flask import Flask, request, jsonify
from tqdm import tqdm

from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

logger = logging.getLogger(__name__)


@dataclass
class RubricCriterion:
    """å•ä¸ªè¯„ä»·æ ‡å‡†"""
    description: str
    weight: int


@dataclass
class EvaluationRequest:
    """è¯„ä»·è¯·æ±‚"""
    rubric: List[Dict[str, Any]]
    question: str
    response: str


@dataclass
class EvaluationResult:
    """è¯„ä»·ç»“æœ - åŒ…å«RubricCriterionä¿¡æ¯å’Œè¯„åˆ†"""
    description: str
    weight: int
    score: float


class RubricRewardService:
    """Rubric å¥–åŠ±æœåŠ¡"""
    
    def __init__(
        self,
        model_path: str = ".../Qwen3-32B",
        sys_prompt_path: str = ".../bbl_reward/prompts/eva_resp_with_rubric_sys_new.txt",
        user_prompt_path: str = ".../bbl_reward/prompts/eva_resp_with_rubric_user_new.txt",
        likert_sys_prompt_path: str = ".../bbl_reward/prompts/eva_likert_sys.txt",
        likert_user_prompt_path: str = ".../bbl_reward/prompts/eva_likert_users.txt",
        outcome_sys_prompt_path: str = ".../bbl_reward/prompts/eva_outcome_sys.txt",
        outcome_user_prompt_path: str = ".../bbl_reward/prompts/eva_outcome_user.txt",
        gpu_memory_utilization: float = 0.9,
        tensor_parallel_size: int = 2
    ):
        """
        åˆå§‹åŒ– Rubric Reward Service
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            sys_prompt_path: ç³»ç»Ÿæç¤ºæ–‡ä»¶è·¯å¾„
            user_prompt_path: ç”¨æˆ·æç¤ºæ–‡ä»¶è·¯å¾„
            likert_sys_prompt_path: Likertè¯„åˆ†ç³»ç»Ÿæç¤ºæ–‡ä»¶è·¯å¾„
            likert_user_prompt_path: Likertè¯„åˆ†ç”¨æˆ·æç¤ºæ–‡ä»¶è·¯å¾„
            gpu_memory_utilization: GPUå†…å­˜ä½¿ç”¨ç‡
            tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°
        """
        self.model_path = model_path
        self.sys_prompt_path = sys_prompt_path
        self.user_prompt_path = user_prompt_path
        self.likert_sys_prompt_path = likert_sys_prompt_path
        self.likert_user_prompt_path = likert_user_prompt_path
        self.outcome_sys_prompt_path = outcome_sys_prompt_path
        self.outcome_user_prompt_path = outcome_user_prompt_path
        
        # åŠ è½½æç¤ºæ¨¡æ¿
        self.system_prompt = self._load_prompt_template(sys_prompt_path)
        self.user_prompt_template = self._load_prompt_template(user_prompt_path)
        
        # åŠ è½½ Likert è¯„åˆ†æç¤ºæ¨¡æ¿
        self.likert_system_prompt = self._load_prompt_template(likert_sys_prompt_path)
        self.likert_user_prompt_template = self._load_prompt_template(likert_user_prompt_path)

        # åŠ è½½ Outcome è¯„åˆ†æç¤ºæ¨¡æ¿
        self.outcome_system_prompt = self._load_prompt_template(outcome_sys_prompt_path)
        self.outcome_user_prompt_template = self._load_prompt_template(outcome_user_prompt_path)
        
        # åˆå§‹åŒ– tokenizer
        logger.info(f"Loading tokenizer from {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # åˆå§‹åŒ– vLLM
        logger.info(f"Loading model from {model_path}")
        self.llm = LLM(
            model=model_path,
            gpu_memory_utilization=gpu_memory_utilization,
            tensor_parallel_size=tensor_parallel_size,
        )
        
        # é‡‡æ ·å‚æ•°
        self.sampling_params = SamplingParams(
            temperature=0.1,
            top_p=0.9,
            max_tokens=16
        )
        
        logger.info("RubricRewardService initialized successfully")
    
    def _load_prompt_template(self, file_path: str) -> str:
        """åŠ è½½æç¤ºæ¨¡æ¿"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            logger.error(f"Error loading prompt template from {file_path}: {e}")
            raise
    
    def _parse_rubric_criteria(self, rubric: List[Dict[str, Any]]) -> List[RubricCriterion]:
        """è§£æ rubric æ ‡å‡†"""
        criteria = []
        for item in rubric:
            try:
                criterion = RubricCriterion(
                    description=item.get("description", ""),
                    weight=int(item.get("weight", 1))
                )
                criteria.append(criterion)
            except Exception as e:
                logger.warning(f"Error parsing rubric item {item}: {e}")
                print()
        return criteria
    
    def _create_single_prompt(
        self,
        question: str,
        response: str,
        criterion: RubricCriterion
    ) -> List[Dict[str, str]]:
        """ä¸ºå•ä¸ªæ ‡å‡†åˆ›å»ºæç¤º"""
        # æ„å»º single_rubric_criterion
        single_rubric_criterion = f"{criterion.description}"
        
        # å¡«å……ç”¨æˆ·æç¤ºæ¨¡æ¿ï¼Œä½¿ç”¨æ–°çš„æ ¼å¼åŒ–è¯­æ³•
        user_prompt = self.user_prompt_template.format(
            prompt=question,
            response=response,
            single_rubric_criterion=single_rubric_criterion
        )
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return messages
    
    def _extract_rating_from_response(self, response_text: str) -> Optional[float]:
        """ä»æ¨¡å‹å“åº”ä¸­æå–è¯„åˆ†"""
        try:
            # å°è¯•åŒ¹é… JSON å—
            json_pattern = r'```json\s*\n?(.*?)\n?```'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1).strip()
                try:
                    data = json.loads(json_str)
                    rating = data.get("rating")
                    if rating is not None:
                        return float(rating)
                except json.JSONDecodeError:
                    pass
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»…åŒ¹é… 0 æˆ– 1
            rating_pattern = r'"rating"\s*:\s*([01])'
            rating_match = re.search(rating_pattern, response_text)
            if rating_match:
                return float(rating_match.group(1))
                
        except Exception as e:
            logger.warning(f"Error extracting rating from response: {e}")
        
        logger.warning(f"Could not extract rating from response: {response_text[:200]}...")
        return None
    
    def _format_messages_for_vllm(self, messages: List[Dict[str, str]]) -> str:
        """å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸º vLLM å¯æ¥å—çš„æ ¼å¼"""
        formatted = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True,
            enable_thinking=False
        )
        return formatted
    
    def evaluate_batch(self, request: EvaluationRequest) -> List[EvaluationResult]:
        """æ‰¹é‡è¯„ä»·"""
        # try:
        # è§£æ rubric æ ‡å‡†
        criteria = self._parse_rubric_criteria(request.rubric)
        if not criteria:
            logger.error("No valid criteria found in rubric")
            return []
        
        # ä¸ºæ¯ä¸ªæ ‡å‡†åˆ›å»ºæç¤º
        prompts = []
        for criterion in criteria:
            messages = self._create_single_prompt(
                request.question,
                request.response,
                criterion
            )
            formatted_prompt = self._format_messages_for_vllm(messages)
            prompts.append(formatted_prompt)
        
        logger.info(f"Processing {len(prompts)} evaluation prompts")
        
        # æ‰¹é‡ç”Ÿæˆ
        outputs = self.llm.generate(prompts, self.sampling_params)
        
        # å¤„ç†ç»“æœ
        results = []
        for i, output in enumerate(outputs):
            criterion = criteria[i]
            response_text = output.outputs[0].text
            
            # æå–è¯„åˆ†
            rating = self._extract_rating_from_response(response_text)
            
            if rating is not None:
                # ç¡®ä¿è¯„åˆ†åœ¨ 1-10 èŒƒå›´å†…
                rating = max(0, min(1, rating))
                result = EvaluationResult(
                    description=criterion.description,
                    weight=criterion.weight,
                    score=rating
                )
                results.append(result)
                logger.info(f"des '{criterion.description}': score={rating}, weight={criterion.weight}")
            else:
                # é»˜è®¤åˆ†æ•°
                logger.warning(f"Failed to extract rating for criterion '{criterion.description}', using default score 1.0")
                result = EvaluationResult(
                    description=criterion.description,
                    weight=criterion.weight,
                    score=0
                )
                results.append(result)
        
        return results
            
        # except Exception as e:
        #     logger.error(f"Error in evaluate_batch: {e}")
        #     return []
    
    def process_request(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªè¯·æ±‚
        
        Args:
            data: æ ¼å¼ä¸º {rubric: rubric, question: question, response: response}
            
        Returns:
            List[Dict]: æ ¼å¼ä¸º [{description: ..., weight: ..., score: ...}, ...]
        """
        # try:
        request = EvaluationRequest(
            rubric=data.get("rubric", []),
            question=data.get("question", ""),
            response=data.get("response", "")
        )
        
        results = self.evaluate_batch(request)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«å®Œæ•´çš„RubricCriterionä¿¡æ¯å’Œscore
        return [
            {
                "description": result.description, 
                "weight": result.weight,
                "score": result.score
            } 
            for result in results
            ]
            
        # except Exception as e:
        #     logger.error(f"Error processing request: {e}")
        #     return []

    def process_batch_fast(self, requests_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚ - ä¸€æ¬¡æ€§æ¨ç†æ‰€æœ‰prompts
        
        Args:
            requests_list: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚æ ¼å¼ä¸º {rubric: rubric, question: question, response: response}
            
        Returns:
            List[Dict]: æ‰¹é‡ç»“æœï¼Œæ ¼å¼ä¸batch_evaluate_apiç›¸åŒ
        """
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ªè¯·æ±‚...")
        
        # æ”¶é›†æ‰€æœ‰promptså’Œå¯¹åº”çš„æ˜ å°„ä¿¡æ¯
        all_prompts = []
        prompt_mappings = []  # è®°å½•æ¯ä¸ªpromptå¯¹åº”çš„request_idxå’Œcriterion_idx
        
        for req_idx, req_data in enumerate(requests_list):
            # è§£æå½“å‰è¯·æ±‚çš„criteria
            request = EvaluationRequest(
                rubric=req_data.get("rubric", []),
                question=req_data.get("question", ""),
                response=req_data.get("response", "")
            )
            
            criteria = self._parse_rubric_criteria(request.rubric)
            if not criteria:
                continue
            
            # ä¸ºæ¯ä¸ªæ ‡å‡†åˆ›å»ºprompt
            for criterion_idx, criterion in enumerate(criteria):
                messages = self._create_single_prompt(
                    request.question,
                    request.response,
                    criterion
                )
                formatted_prompt = self._format_messages_for_vllm(messages)
                all_prompts.append(formatted_prompt)
                
                # è®°å½•æ˜ å°„å…³ç³»
                prompt_mappings.append({
                    'request_idx': req_idx,
                    'criterion_idx': criterion_idx,
                    'criterion': criterion
                })
        
        print(f"æ€»å…±ç”Ÿæˆ {len(all_prompts)} ä¸ªprompts")
        
        # ä¸€æ¬¡æ€§æ‰¹é‡æ¨ç†æ‰€æœ‰prompts
        if all_prompts:
            outputs = self.llm.generate(all_prompts, self.sampling_params)
            print(f"vLLMç”Ÿæˆäº† {len(outputs)} ä¸ªç»“æœ")
        else:
            outputs = []
        
        # æŒ‰requesté‡æ–°ç»„ç»‡ç»“æœ
        results_by_request = {}
        
        for prompt_idx, (output, mapping) in enumerate(zip(outputs, prompt_mappings)):
            request_idx = mapping['request_idx']
            criterion_idx = mapping['criterion_idx']
            criterion = mapping['criterion']
            
            if request_idx not in results_by_request:
                results_by_request[request_idx] = []
            
            # æå–è¯„åˆ†
            response_text = output.outputs[0].text
            rating = self._extract_rating_from_response(response_text)
            
            if rating is not None:
                rating = max(0, min(1, rating))
            else:
                logger.warning(f"Failed to extract rating for criterion '{criterion.description}', using default score 5.0")
                rating = 0
            
            # æ·»åŠ ç»“æœ
            result = {
                "description": criterion.description,
                "weight": criterion.weight,
                "score": rating
            }
            results_by_request[request_idx].append(result)
        
        # æ„å»ºæœ€ç»ˆè¿”å›æ ¼å¼
        final_results = []
        for req_idx in range(len(requests_list)):
            if req_idx in results_by_request:
                results = results_by_request[req_idx]
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_score = sum(result["score"] * result["weight"] for result in results)
                # åˆ†æ¯è®¡ç®—ï¼šæ­£çš„æƒé‡*10 + è´Ÿçš„æƒé‡*1
                total_weight = sum(result["weight"] * 1.0 for result in results)
                weighted_average = total_score / total_weight if total_weight > 0 else 0.0
                
                # è®¡ç®— score_list (æ¯ä¸ª rubric çš„ score * weight)
                score_list = [float(result["score"]) * float(result["weight"]) for result in results]
                # å°† score_list åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… numpy æ•°ç»„è½¬æ¢é”™è¯¯
                import json
                score_list_json = json.dumps(score_list)
                
                final_results.append({
                    "index": req_idx,
                    "results": results,
                    "score_list": score_list_json,
                    "summary": {
                        "weighted_average": weighted_average
                    }
                })
            else:
                # æ²¡æœ‰æœ‰æ•ˆcriteriaçš„è¯·æ±‚
                final_results.append({
                    "index": req_idx,
                    "error": "æ— æœ‰æ•ˆçš„è¯„ä»·æ ‡å‡†"
                })
        
        print(f"å¿«é€Ÿæ‰¹é‡å¤„ç†å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results
            
        # except Exception as e:
        #     logger.error(f"å¿«é€Ÿæ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        #     import traceback
        #     traceback.print_exc()
        #     return []

    def _create_likert_prompt(
        self,
        question: str,
        reference_answer: str,
        response: str
    ) -> List[Dict[str, str]]:
        """ä¸º Likert è¯„åˆ†åˆ›å»ºæç¤º"""
        # å¡«å……ç”¨æˆ·æç¤ºæ¨¡æ¿
        user_prompt = self.likert_user_prompt_template.format(
            prompt=question,
            reference=reference_answer,
            response=response
        )
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "system", "content": self.likert_system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return messages

    def _create_outcome_prompt(
        self,
        question: str,
        groundtruth: str,
        response: str
    ) -> List[Dict[str, str]]:
        """ä¸º Outcome 0/1 è¯„åˆ†åˆ›å»ºæç¤º"""
        # åŸºæœ¬å­—æ®µæ ¡éªŒï¼šä»»ä¸€ä¸ºç©ºç›´æ¥æŠ¥é”™
        empty_fields = []
        if not question:
            empty_fields.append("question")
        if not groundtruth:
            empty_fields.append("groundtruth")
        if not response:
            empty_fields.append("response")
        if empty_fields:
            raise ValueError(f"Outcomeè¯„ä¼°æ‰€éœ€å­—æ®µä¸ºç©º: {empty_fields}")

        user_prompt = self.outcome_user_prompt_template.format(
            prompt=question,
            groundtruth=groundtruth,
            response=response
        )

        messages = [
            {"role": "system", "content": self.outcome_system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        return messages

    def process_outcome_batch_fast(self, requests_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿæ‰¹é‡å¤„ç† Outcome 0/1 è¯„åˆ†è¯·æ±‚
        
        Args:
            requests_list: åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {groundtruth: str, question: str, response: str}
        Returns:
            List[Dict]: å½¢å¦‚ [{"index": i, "score": float}, ...]ï¼Œåˆ†æ•°èŒƒå›´[0,1]
        """
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ª Outcome è¯„åˆ†è¯·æ±‚...")

        all_prompts = []

        for req_idx, req_data in enumerate(requests_list):
            question = req_data.get("question", "")
            groundtruth = req_data.get("groundtruth", "")
            response = req_data.get("response", "")

            messages = self._create_outcome_prompt(question, groundtruth, response)
            formatted_prompt = self._format_messages_for_vllm(messages)
            all_prompts.append(formatted_prompt)

        print(f"æ€»å…±ç”Ÿæˆ {len(all_prompts)} ä¸ª Outcome è¯„åˆ† prompts")
        print(f"example prompt: {all_prompts[0]}")

        if all_prompts:
            outputs = self.llm.generate(all_prompts, self.sampling_params)
            print(f"vLLMç”Ÿæˆäº† {len(outputs)} ä¸ªç»“æœ")
        else:
            outputs = []

        final_results = []
        for req_idx, output in enumerate(outputs):
            response_text = output.outputs[0].text
            rating = self._extract_rating_from_response(response_text)

            if rating is not None:
                rating = max(0.0, min(1.0, rating))
            else:
                logger.warning(f"Failed to extract Outcome rating for request {req_idx}, using default score 0.0")
                rating = 0.0

            final_results.append({
                "index": req_idx,
                "score": rating
            })

        print(f"å¿«é€Ÿ Outcome æ‰¹é‡å¤„ç†å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results

    def process_likert_batch_fast(self, requests_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿæ‰¹é‡å¤„ç† Likert è¯„åˆ†è¯·æ±‚
        
        Args:
            requests_list: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚æ ¼å¼ä¸º {reference_answer: str, question: str, response: str}
            
        Returns:
            List[Dict]: æ‰¹é‡ç»“æœï¼Œæ ¼å¼ä¸º [{"index": i, "score": float}, ...]
        """
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ª Likert è¯„åˆ†è¯·æ±‚...")
        
        # æ”¶é›†æ‰€æœ‰prompts
        all_prompts = []
        
        for req_idx, req_data in enumerate(requests_list):
            question = req_data.get("question", "")
            reference_answer = req_data.get("reference_answer", "")
            response = req_data.get("response", "")
            
            # åˆ›å»º Likert è¯„åˆ†æç¤º
            messages = self._create_likert_prompt(question, reference_answer, response)
            formatted_prompt = self._format_messages_for_vllm(messages)
            all_prompts.append(formatted_prompt)
        
        print(f"æ€»å…±ç”Ÿæˆ {len(all_prompts)} ä¸ª Likert è¯„åˆ† prompts")
        
        # ä¸€æ¬¡æ€§æ‰¹é‡æ¨ç†æ‰€æœ‰prompts
        if all_prompts:
            outputs = self.llm.generate(all_prompts, self.sampling_params)
            print(f"vLLMç”Ÿæˆäº† {len(outputs)} ä¸ªç»“æœ")
        else:
            outputs = []
        
        # å¤„ç†ç»“æœ
        final_results = []
        
        for req_idx, output in enumerate(outputs):
            # æå–è¯„åˆ†
            response_text = output.outputs[0].text
            rating = self._extract_rating_from_response(response_text)
            
            if rating is not None:
                rating = max(1.0, min(10.0, rating))
            else:
                logger.warning(f"Failed to extract Likert rating for request {req_idx}, using default score 5.0")
                rating = 5.0
            
            final_results.append({
                "index": req_idx,
                "score": rating
            })
        
        print(f"å¿«é€Ÿ Likert æ‰¹é‡å¤„ç†å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results


# å…¨å±€æœåŠ¡å®ä¾‹
service = None

def init_service():
    """åˆå§‹åŒ–æœåŠ¡"""
    global service
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Rubric Reward Service...")
    service = RubricRewardService()
    logger.info("âœ… æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

@app.route('/evaluate', methods=['POST'])
def evaluate_api():
    """è¯„ä»·API"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "ç¼ºå°‘JSONæ•°æ®"}), 400
        
        # éªŒè¯å¿…è¦å­—æ®µ
        required_keys = ['rubric', 'question', 'response']
        missing_keys = [key for key in required_keys if key not in data]
        
        if missing_keys:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}"}), 400
        
        # å¤„ç†è¯·æ±‚
        results = service.process_request(data)
        
        if results:
            return jsonify(results)
        else:
            return jsonify({"error": "è¯„ä»·å¤±è´¥"}), 500
            
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/batch_evaluate', methods=['POST'])
def batch_evaluate_api():
    """æ‰¹é‡è¯„ä»·API - ä¸²è¡Œå¤„ç†"""
    try:
        data = request.get_json()
        if not data or 'requests' not in data:
            return jsonify({"error": "ç¼ºå°‘requestså­—æ®µ"}), 400
        
        requests_list = data['requests']
        results_list = []
        
        print(f"å¼€å§‹å¤„ç† {len(requests_list)} ä¸ªæ‰¹é‡è¯„ä»·è¯·æ±‚...")
        
        for i, req_data in tqdm(enumerate(requests_list), total=len(requests_list), desc="å¤„ç†batchè¯·æ±‚"):
            # éªŒè¯æ¯ä¸ªè¯·æ±‚çš„å¿…è¦å­—æ®µ
            required_keys = ['rubric', 'question', 'response']
            missing_keys = [key for key in required_keys if key not in req_data]
            
            if missing_keys:
                results_list.append({
                    "index": i,
                    "error": f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}"
                })
                continue
            
            # å¤„ç†è¯·æ±‚
            results = service.process_request(req_data)
            
            if results:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_score = sum(result["score"] * result["weight"] for result in results)
                total_weight = sum(result["weight"] * (10 if result["weight"] > 0 else 1) for result in results)
                weighted_average = total_score / total_weight if total_weight > 0 else 0.0
                
                # è®¡ç®— score_list (æ¯ä¸ª rubric çš„ score * weight)
                score_list = [float(result["score"]) * float(result["weight"]) for result in results]
                # å°† score_list åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… numpy æ•°ç»„è½¬æ¢é”™è¯¯
                import json
                score_list_json = json.dumps(score_list)
                
                results_list.append({
                    "index": i,
                    "results": results,
                    "score_list": score_list_json,
                    "summary": {
                        "weighted_average": weighted_average
                    }
                })
            else:
                results_list.append({
                    "index": i,
                    "error": "è¯„ä»·å¤±è´¥"
                })
        
        return jsonify({"batch_results": results_list})
        
    except Exception as e:
        logger.error(f"å¤„ç†æ‰¹é‡è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/batch_evaluate_fast', methods=['POST'])
def batch_evaluate_fast_api():
    """å¿«é€Ÿæ‰¹é‡è¯„ä»·API - å¹¶è¡Œå¤„ç†"""
    try:
        data = request.get_json()
        if not data or 'requests' not in data:
            return jsonify({"error": "ç¼ºå°‘requestså­—æ®µ"}), 400
        
        requests_list = data['requests']
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ªè¯·æ±‚...")
        
        # éªŒè¯æ‰€æœ‰è¯·æ±‚çš„å¿…è¦å­—æ®µ
        valid_requests = []
        request_indices = []
        
        for i, req_data in enumerate(requests_list):
            required_keys = ['rubric', 'question', 'response']
            missing_keys = [key for key in required_keys if key not in req_data]
            
            if missing_keys:
                # è·³è¿‡æ— æ•ˆè¯·æ±‚ï¼Œç¨åå¡«å……é”™è¯¯ç»“æœ
                continue
            else:
                valid_requests.append(req_data)
                request_indices.append(i)
        
        print(f"æœ‰æ•ˆè¯·æ±‚æ•°é‡: {len(valid_requests)}")
        
        if not valid_requests:
            return jsonify({"error": "æ²¡æœ‰æœ‰æ•ˆçš„è¯·æ±‚"}), 400
        
        # ä½¿ç”¨æ–°çš„æ‰¹é‡å¤„ç†æ–¹æ³•
        batch_results = service.process_batch_fast(valid_requests)
        
        # æ„å»ºæœ€ç»ˆç»“æœåˆ—è¡¨
        results_list = [None] * len(requests_list)
        
        # å¡«å……æœ‰æ•ˆè¯·æ±‚çš„ç»“æœ
        for result_idx, request_idx in enumerate(request_indices):
            if result_idx < len(batch_results):
                results_list[request_idx] = batch_results[result_idx]
        
        # å¡«å……æ— æ•ˆè¯·æ±‚çš„é”™è¯¯ä¿¡æ¯
        for i, req_data in enumerate(requests_list):
            if results_list[i] is None:
                required_keys = ['rubric', 'question', 'response']
                missing_keys = [key for key in required_keys if key not in req_data]
                results_list[i] = {
                    "index": i,
                    "error": f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}"
                }
        
        return jsonify({"batch_results": results_list})
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿæ‰¹é‡å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/likert_evaluate_fast', methods=['POST'])
def likert_evaluate_fast_api():
    """å¿«é€Ÿæ‰¹é‡ Likert è¯„åˆ†API"""
    try:
        data = request.get_json()
        if not data or 'requests' not in data:
            return jsonify({"error": "ç¼ºå°‘requestså­—æ®µ"}), 400
        
        requests_list = data['requests']
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ª Likert è¯„åˆ†è¯·æ±‚...")
        
        # éªŒè¯æ‰€æœ‰è¯·æ±‚çš„å¿…è¦å­—æ®µ
        valid_requests = []
        request_indices = []
        
        for i, req_data in enumerate(requests_list):
            required_keys = ['reference_answer', 'question', 'response']
            missing_keys = [key for key in required_keys if key not in req_data]
            
            if missing_keys:
                # è·³è¿‡æ— æ•ˆè¯·æ±‚ï¼Œç¨åå¡«å……é”™è¯¯ç»“æœ
                continue
            else:
                valid_requests.append(req_data)
                request_indices.append(i)
        
        print(f"æœ‰æ•ˆ Likert è¯„åˆ†è¯·æ±‚æ•°é‡: {len(valid_requests)}")
        
        if not valid_requests:
            return jsonify({"error": "æ²¡æœ‰æœ‰æ•ˆçš„è¯·æ±‚"}), 400
        
        # ä½¿ç”¨æ–°çš„ Likert æ‰¹é‡å¤„ç†æ–¹æ³•
        batch_results = service.process_likert_batch_fast(valid_requests)
        
        # æ„å»ºæœ€ç»ˆç»“æœåˆ—è¡¨
        results_list = [None] * len(requests_list)
        
        # å¡«å……æœ‰æ•ˆè¯·æ±‚çš„ç»“æœ
        for result_idx, request_idx in enumerate(request_indices):
            if result_idx < len(batch_results):
                results_list[request_idx] = batch_results[result_idx]
        
        # å¡«å……æ— æ•ˆè¯·æ±‚çš„é”™è¯¯ä¿¡æ¯
        for i, req_data in enumerate(requests_list):
            if results_list[i] is None:
                required_keys = ['reference_answer', 'question', 'response']
                missing_keys = [key for key in required_keys if key not in req_data]
                results_list[i] = {
                    "index": i,
                    "error": f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}"
                }
        
        return jsonify({"batch_results": results_list})
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿ Likert æ‰¹é‡å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/outcome_evaluate_fast', methods=['POST'])
def outcome_evaluate_fast_api():
    """å¿«é€Ÿæ‰¹é‡ Outcome è¯„åˆ†API"""
    try:
        data = request.get_json()
        if not data or 'requests' not in data:
            return jsonify({"error": "ç¼ºå°‘requestså­—æ®µ"}), 400
        
        requests_list = data['requests']
        print(f"å¼€å§‹å¿«é€Ÿæ‰¹é‡å¤„ç† {len(requests_list)} ä¸ª Outcome è¯„åˆ†è¯·æ±‚...")
        
        # éªŒè¯æ‰€æœ‰è¯·æ±‚çš„å¿…è¦å­—æ®µ
        valid_requests = []
        request_indices = []
        
        for i, req_data in enumerate(requests_list):
            required_keys = ['groundtruth', 'question', 'response']
            missing_or_empty = [key for key in required_keys if (key not in req_data or not req_data.get(key))]
            
            if missing_or_empty:
                # è·³è¿‡æ— æ•ˆè¯·æ±‚ï¼Œç¨åå¡«å……é”™è¯¯ç»“æœ
                continue
            else:
                valid_requests.append(req_data)
                request_indices.append(i)
        
        print(f"æœ‰æ•ˆ Outcome è¯„åˆ†è¯·æ±‚æ•°é‡: {len(valid_requests)}")
        
        if not valid_requests:
            return jsonify({"error": "æ²¡æœ‰æœ‰æ•ˆçš„è¯·æ±‚"}), 400
        
        # ä½¿ç”¨ Outcome æ‰¹é‡å¤„ç†æ–¹æ³•
        batch_results = service.process_outcome_batch_fast(valid_requests)
        
        # æ„å»ºæœ€ç»ˆç»“æœåˆ—è¡¨
        results_list = [None] * len(requests_list)
        
        # å¡«å……æœ‰æ•ˆè¯·æ±‚çš„ç»“æœ
        for result_idx, request_idx in enumerate(request_indices):
            if result_idx < len(batch_results):
                results_list[request_idx] = batch_results[result_idx]
        
        # å¡«å……æ— æ•ˆè¯·æ±‚çš„é”™è¯¯ä¿¡æ¯
        for i, req_data in enumerate(requests_list):
            if results_list[i] is None:
                required_keys = ['groundtruth', 'question', 'response']
                missing_or_empty = [key for key in required_keys if (key not in req_data or not req_data.get(key))]
                results_list[i] = {
                    "index": i,
                    "error": f"ç¼ºå°‘æˆ–ä¸ºç©ºçš„å¿…è¦å­—æ®µ: {missing_or_empty}"
                }
        
        return jsonify({"batch_results": results_list})
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿ Outcome æ‰¹é‡å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_api():
    """å¥åº·æ£€æŸ¥"""
    try:
        if service is None:
            return jsonify({"status": "error", "message": "æœåŠ¡æœªåˆå§‹åŒ–"}), 503
        
        return jsonify({
            "status": "healthy",
            "service": "rubric_reward_service",
            "backend": "vllm",
            "model_path": service.model_path
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/', methods=['GET'])
def root():
    """æ ¹è·¯å¾„"""
    return jsonify({
        "service": "Rubric Reward Service API",
        "version": "1.0.0",
        "endpoints": {
            "/evaluate": "POST - å•æ¬¡è¯„ä»·",
            "/batch_evaluate": "POST - æ‰¹é‡è¯„ä»· (ä¸²è¡Œå¤„ç†)", 
            "/batch_evaluate_fast": "POST - å¿«é€Ÿæ‰¹é‡è¯„ä»· (å¹¶è¡Œå¤„ç†)",
            "/likert_evaluate_fast": "POST - å¿«é€Ÿæ‰¹é‡ Likert è¯„åˆ†",
            "/outcome_evaluate_fast": "POST - å¿«é€Ÿæ‰¹é‡ Outcome 0/1 è¯„åˆ†",
            "/health": "GET - å¥åº·æ£€æŸ¥"
        }
    })

def main():
    """ä¸»å‡½æ•° - å¯åŠ¨FlaskæœåŠ¡"""
    # åˆå§‹åŒ–æœåŠ¡
    init_service()
    
    # å¯åŠ¨FlaskæœåŠ¡
    logger.info("ğŸŒ å¯åŠ¨FlaskæœåŠ¡åœ¨ http://0.0.0.0:8003")
    logger.info("ğŸ“– APIæ–‡æ¡£:")
    logger.info("  POST /evaluate - å•æ¬¡è¯„ä»·")
    logger.info("  POST /batch_evaluate - æ‰¹é‡è¯„ä»· (ä¸²è¡Œå¤„ç†)")
    logger.info("  POST /batch_evaluate_fast - å¿«é€Ÿæ‰¹é‡è¯„ä»· (å¹¶è¡Œå¤„ç†)")
    logger.info("  POST /likert_evaluate_fast - å¿«é€Ÿæ‰¹é‡ Likert è¯„åˆ†")
    logger.info("  GET /health - å¥åº·æ£€æŸ¥")
    
    app.run(host='0.0.0.0', port=8003, debug=False, threaded=True)

if __name__ == "__main__":
    main()
